{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOvJ5mGFonb0"
      },
      "outputs": [],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "la6DDelOowMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "dY5Nwr0jmf8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-gpu"
      ],
      "metadata": {
        "id": "DAtJ7TlibsLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "import requests\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "import pickle\n",
        "\n",
        "# from langchain import OpenAI\n",
        "from langchain.chains import VectorDBQAWithSourcesChain\n",
        "import argparse"
      ],
      "metadata": {
        "id": "o6NJZxXBmP3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the llm\n",
        "llm = OpenAI(openai_api_key=\"ADD_YOUR_API\", temperature=0)"
      ],
      "metadata": {
        "id": "KNTA_2dvnkE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to web scrape and to remove unnecessary parts\n",
        "def extract_text_from(url):\n",
        "    \"\"\"Returns the text content of the given webpage.\"\"\"\n",
        "    html = requests.get(url).text\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    return '\\n'.join(line for line in lines if line)"
      ],
      "metadata": {
        "id": "FVUAF5bnNvJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample URL to be passed to the above function\n",
        "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "\n",
        "pages = extract_text_from(url)"
      ],
      "metadata": {
        "id": "Ycy8XOWV-9DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splits based on characters and measure chunk length by number of characters.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
        "docs = text_splitter.split_text(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjTApBTCDYAv",
        "outputId": "dcebe0b5-c5f1-4a10-ff95-b170985a4ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2078, which is longer than the specified 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "y5esLGDUUuUS",
        "outputId": "e96e0af8-145e-4cfc-daa8-9eff965bc860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#llm\\nIf you want to learn how to create embeddings of your website and how to use a question answering bot to answer questions which are covered by your website, then you are in the right spot.\\nThe Github repository which contains all the code of this blog entry can be found here.\\nIt was trending on Hacker news on March 22nd and you can check out the disccussion here.\\nWe will approach this goal as follows:\\nRead more →\\nInteractive visualization of stable diffusion image embeddings\\nFebruary 26, 2023\\n— Written by Marc Päpper\\n— ⏰  7 min read\\n#machinelearning\\n#visualization\\nA great site to discover images generated by stable diffusion (or their custom model called aperture) is Lexica.art.\\nLexica provides an API which can be used to query images matching some keyword / topic. The API returns image URLs, sizes and other things like the prompt used to generate the image and its seed.\\nThe goal of this blog post is to visualize the similarity of images from different categories in an interactive plot which can be explored in the browser.\\nRead more →\\nOlder posts\\n→\\nI help you listen through the noise in machine learning:\\nConnect with me on twitter ->\\nConnect with @mpaepper\\nRead. Hack. Learn. Repeat. © 11111100011 Marc Päpper\\nImprint\\nPrivacy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector store of these embeddings\n",
        "store = FAISS.from_texts(docs, OpenAIEmbeddings())\n",
        "with open(\"faiss_store.pkl\", \"wb\") as f:\n",
        "    pickle.dump(store, f)"
      ],
      "metadata": {
        "id": "FvND1HnhDYMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking questions\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Website Q&A')\n",
        "parser.add_argument('question', type=str, help= input('Your question for Website'))\n",
        "\n",
        "# parser.add_argument('question', type=str, help='Your question for Paepper.com')\n",
        "# promt_user_question=prompt.format(question=user_question)\n",
        "args = parser.parse_args()\n",
        "\n",
        "with open(\"faiss_store.pkl\", \"rb\") as f:\n",
        "    store = pickle.load(f)\n",
        "\n",
        "chain = VectorDBQAWithSourcesChain.from_llm(\n",
        "            llm=OpenAI(temperature=0), vectorstore=store)\n",
        "result = chain({\"question\": args.question})\n",
        "\n",
        "print(f\"Answer: {result['answer']}\")"
      ],
      "metadata": {
        "id": "2T6SsvrlgIUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}